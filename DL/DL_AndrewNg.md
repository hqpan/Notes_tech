[toc]

# 版权声明

- 深度学习系列学习笔记来源于：
  - Aston Zhang，Zack C. Lipton，李沐和 Alex J. Smola 所著 *Dive into Deep Learning* [1]；
  - Andrew Ng 教授在 Coursera 网站上所授课程《Deep Learning》[2]；；
- 该系列笔记不以盈利为目的，仅用于个人学习、课后复习及科学研究；
- 如有侵权，请与本人联系（hqpan@foxmail.com），经核实后即刻删除；
- 本文采用 [署名-非商业性使用-禁止演绎 4.0 国际 (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh) 协议发布；

# 1. 深度学习概论

## 1.1 基本概念

- 隐藏单元：hidden units；
- CNN 和 RNN：
  - CNN：常用于处理图像数据；
  - RNN：常用于处理一维时序数据；
- 结构化数据和非结构化数据：
  - 结构化数据：E.g. 数据库中的一张表；
  - 非结构化数据：文本、音频、图像；
- 影响最终性能的因素：
  - 当训练集规模较小时，最终性能取决于开发人员手工设计特征的能力（hand engineer features）及算法处理方面的细节，此时传统机器学习方法的性能可能会超过神经网络；
  - 当训练集规模较大时，最终性能取决于算法模型自身，此时大规模神经网络的性能远胜于传统机器学习方法；

## 1.2 符号含义

- $a^{[i]}$：第$i$层中的激活值向量；
  - $a^{[0]}$即为输入特征$x$；
  - $a^{[L]}=\widehat{y}$；
  - $a^{[i]}_j$：第$i$层中的激活值向量，其中的第$j$个元素取值；
- $a^{[l]}$：第$l$层中的激活函数值；
- $g^{[l]}(x)$：第$l$层中选取的激活函数；
- L：神经网络层数；
- m：训练样本数量，即数据集规模；
  - $m_{train}$：训练集样本数；
  - $m_{test}$：测试集样本数；
- $n^{[l]}$：第$l$层中隐藏单元的数量；
- $n_x$：特征向量$x$的维度，有时简写为$n$；
- W、b：
  - W：权重参数；
  - b：阈值；
- $(x,y)$：表示一个样本；
- $(x^{(i)},y^{(i)})$：表示数据集中的第$i$个样本；
- $\widehat{y}$：模型输出的对$y$的预测值；

# 2. 神经网络基础

- **计算图**的计算求解：
  - 前向传播；
  - 反向传播；
- 向量化相较于循环语句，计算速度快；
- Python 中的广播机制；

# 3. 浅层神经网络

## 3.1 神经网络的表示

- 神经网络的表示：统计神经网络层数时，不统计输入层，仅统计隐含层和输出层；
  - 输入层；
  - 隐藏层：可为不同的隐藏层选择不同的激活函数；
  - 输出层；
- 激活值：输入特征的数值，网络中不同层的值；

## 3.2 激活函数

- 常见的非线性激活函数：
  - Sigmoid 函数；
  - tanh 函数；
  - ReLU 函数；
  - the Leaky ReLU 函数；
- Q：为什么神经网络中需要使用非线性激活函数？
  - A：若使用线性激活函数，E.g. $y=x$，则$\widehat{y}$将是所有输入特征的线性组合，性能等价于没有任何隐藏层的网络，即线性隐藏层没有价值；
  - 注意：若处理线性回归问题，$y$是一个实数，则输出层可使用线性激活函数；
- Sigmoid 函数：$\sigma(x)=\frac{1}{1+e^{-x}}$，$x\in(-\infty,+\infty)$；
  - 功能：将$(-\infty,+\infty)$之间的值映射到区间$(0,1)$上；
  - 特点：在$-\infty$和$+\infty$方向上梯度为 0；
  - 使用梯度下降法时，若梯度接近于 0，参数更新将会减缓；
  - 注意：Sigmoid 函数适用于二分类的输出层；
- tanh 函数：双曲正切函数；
  - $tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}$；
  - 值域为$(-1,1)$；
  - 使用梯度下降法时，若梯度接近于 0，参数更新将会减缓；
  - 注意：在大多数情况下，tanh 函数的性能优于 Sigmoid 函数，前者的平均值更接近于 0；
- ReLU函数：Rectified Linear Unit，线性修正单元；
  - $y=max(0,x)$；
  - 修正：指其取值不小于 0；
  - 对于大于 0 的输入，其梯度始终为 1；
  - 将 sigmoid 函数替换为 ReLU 函数，将使梯度下降算法运行得更快，神经网络学习速度更快；
- the Leaky ReLU 函数：带泄露的 ReLU 函数；
  - ReLU 函数的缺陷：当$x<0$时，函数的导数为 0；
  - 当$x<0$时，the Leaky ReLU 函数是一个非常接近于 0 的正值；
- 选取激活函数的经验法则：
  - Sigmoid 函数适用于二分类的输出层；
  - 在其他情况下，对于其它各隐藏层，应使用 ReLU 函数；

## 3.3 神经网络参数初始化

- 神经网络参数初始化：应随机初始化权重参数矩阵$W$，而非将其全置为 0；
  - 若将各隐藏单元参数均初始化为 0，则所有隐藏单元均是对称的，均在计算完全相同的函数；
  - 若需要使得两个隐藏单元计算不同的函数，则应随机初始化；
  - 一般将参数初始化为较小的随机值，若将其初始化为较大的随机值，则有可能落入 Sigmoid 函数或 tanh 函数中导数接近于 0 的区域；
  - 注意：对于参数$b$，可将其全部初始化为 0；

# 4. 深层神经网络

## 4.1 前向传播与反向传播

- 在正向传播过程中，将计算所得的$W^{[l]}$、$b^{[l]}$、$z^{[l]}$缓存下来，供反向传播时使用；
- 在反向传播中，输入为$da^{[l]}$和在正向传播计算中得到的$z^{[l]}$，输出为$da^{[l-1]}$、$dW^{[l]}$、$db^{[l]}$，以供梯度下降算法使用，更新$W^{[l]}$和$b^{[l]}$；

## 4.2 核对矩阵维数

- 第$l$层权重参数矩阵$W^{[l]}$的维度应为$n^{[l]}\times n^{[l-1]}$；
  - 理解：将前一隐藏层的激活值向量转换为后一隐藏层的激活值向量；
  - 反向传播时，$dW^{[l]}$的维度与$W^{[l]}$相同；
- 第$l$层参数$b$的维度应为$n^{[l]}\times 1$；
  - 反向传播时，$db$的维度与$b$相同；

## 4.3 参数和超参数

- 参数：通过学习得到；
  - 权重参数$W$；
  - $b$；
- 超参数：超参数需要人为设置，而非学习得到，其将影响学得的参数结果；
  - 学习率$\alpha$；
  - 梯度下降法循环的次数；
  - 隐藏层数$L$；
  - 隐藏层单元数；
  - 每个隐藏层的激活函数类型；
- 选取神经网络层数的方法：
  - 先尝试单层、两层的神经网络模型；
  - 将隐藏层层数作为一个超参数，在交叉验证集上评估；
- 选取超参数的方法：设计多组对照实验，观察代价函数在多次迭代过程中的变化规律，筛选最优的超参数；

# Unsolved

- 表征学习：是机器学习中的一类方法，用于自动找出适合表示数据的方式，而深度学习是具有多级表示的表征学习方法，可以逐级表示越来越抽象的概念或模式；

# TODO

- 10月18日前完成 Course 2 & 3；
- 10月25日前完成 Course 4；
- 10月31日前完成 Course 5；
- 11月19日前完成 CS 224n；
- 择机完成李宏毅老师的NLP视频课程；
- 当前进度：Course 1，Week 4已完成；
  - Course 2-5 共计 12 Weeks；
  - 每周学习 4 Weeks 的课程，即每天学习 1 Week，剩余 3 天学习纸质教材；
- 在理论学习期间择机阅读：
  - 《深度学习框架 PyTorch 入门与实践》；
  - 《自然语言处理入门》；
  - 《动手学深度学习》；
- 《动手学深度学习》进度：	
  - 总页数：390 页；
  - 每天 10 页，8 月 30 日完成；
  - 当前进度：Page 24，Chapter 1 已完成；

# References

[1] Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola. Dive into Deep Learning[M]. 人民邮电出版社, 2019. 
[2] https://www.coursera.org/specializations/deep-learning?