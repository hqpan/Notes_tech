[toc]

# 版权声明

- 深度学习系列学习笔记来源于：
  - Aston Zhang，Zack C. Lipton，李沐和 Alex J. Smola 所著 *Dive into Deep Learning* [1]；
  - Andrew Ng 教授在 Coursera 网站上所授课程《Deep Learning》[2]；；
- 该系列笔记不以盈利为目的，仅用于个人学习、课后复习及科学研究；
- 如有侵权，请与本人联系（hqpan@foxmail.com），经核实后即刻删除；
- 本文采用 [署名-非商业性使用-禁止演绎 4.0 国际 (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh) 协议发布；

# 1. 深度学习概论

## 1.1 基本概念

- 隐藏单元：hidden units；
- CNN 和 RNN：
  - CNN：常用于处理图像数据；
  - RNN：常用于处理一维时序数据；
- 结构化数据和非结构化数据：
  - 结构化数据：E.g. 数据库中的一张表；
  - 非结构化数据：文本、音频、图像；
- 影响最终性能的因素：
  - 当训练集规模较小时，最终性能取决于开发人员手工设计特征的能力（hand engineer features）及算法处理方面的细节，此时传统机器学习方法的性能可能会超过神经网络；
  - 当训练集规模较大时，最终性能取决于算法模型自身，此时大规模神经网络的性能远胜于传统机器学习方法；

## 1.2 符号含义

- $a^{[i]}$：第$i$层中的激活值向量；
  - $a^{[0]}$即为输入特征$x$；
  - $a^{[L]}=\widehat{y}$；
  - $a^{[i]}_j$：第$i$层中的激活值向量，其中的第$j$个元素取值；
- $a^{[l]}$：第$l$层中的激活函数值；
- $g^{[l]}(x)$：第$l$层中选取的激活函数；
- L：神经网络层数；
- m：训练样本数量，即数据集规模；
  - $m_{train}$：训练集样本数；
  - $m_{test}$：测试集样本数；
- $n^{[l]}$：第$l$层中隐藏单元的数量；
- $n_x$：特征向量$x$的维度，有时简写为$n$；
- W、b：
  - W：权重参数；
  - b：阈值；
- $(x,y)$：表示一个样本；
- $(x^{(i)},y^{(i)})$：表示数据集中的第$i$个样本；
- $\widehat{y}$：模型输出的对$y$的预测值；

# 2. 神经网络基础

- **计算图**的计算求解：
  - 前向传播；
  - 反向传播；
- 向量化相较于循环语句，计算速度快；
- Python 中的广播机制；

# 3. 浅层神经网络

## 3.1 神经网络的表示

- 神经网络的表示：统计神经网络层数时，不统计输入层，仅统计隐含层和输出层；
  - 输入层；
  - 隐藏层：可为不同的隐藏层选择不同的激活函数；
  - 输出层；
- 激活值：输入特征的数值，网络中不同层的值；

## 3.2 激活函数

- 常见的非线性激活函数：
  - Sigmoid 函数；
  - tanh 函数；
  - ReLU 函数；
  - the Leaky ReLU 函数；
- Q：为什么神经网络中需要使用非线性激活函数？
  - A：若使用线性激活函数，E.g. $y=x$，则$\widehat{y}$将是所有输入特征的线性组合，性能等价于没有任何隐藏层的网络，即线性隐藏层没有价值；
  - 注意：若处理线性回归问题，$y$是一个实数，则输出层可使用线性激活函数；
- Sigmoid 函数：$\sigma(x)=\frac{1}{1+e^{-x}}$，$x\in(-\infty,+\infty)$；
  - 功能：将$(-\infty,+\infty)$之间的值映射到区间$(0,1)$上；
  - 特点：在$-\infty$和$+\infty$方向上梯度为 0；
  - 使用梯度下降法时，若梯度接近于 0，参数更新将会减缓；
  - 注意：Sigmoid 函数适用于二分类的输出层；
- tanh 函数：双曲正切函数；
  - $tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}$；
  - 值域为$(-1,1)$；
  - 使用梯度下降法时，若梯度接近于 0，参数更新将会减缓；
  - 注意：在大多数情况下，tanh 函数的性能优于 Sigmoid 函数，前者的平均值更接近于 0；
- ReLU函数：Rectified Linear Unit，线性修正单元；
  - $y=max(0,x)$；
  - 修正：指其取值不小于 0；
  - 对于大于 0 的输入，其梯度始终为 1；
  - 将 sigmoid 函数替换为 ReLU 函数，将使梯度下降算法运行得更快，神经网络学习速度更快；
- the Leaky ReLU 函数：带泄露的 ReLU 函数；
  - ReLU 函数的缺陷：当$x<0$时，函数的导数为 0；
  - 当$x<0$时，the Leaky ReLU 函数是一个非常接近于 0 的正值；
- 选取激活函数的经验法则：
  - Sigmoid 函数适用于二分类的输出层；
  - 在其他情况下，对于其它各隐藏层，应使用 ReLU 函数；

## 3.3 神经网络参数初始化

- 神经网络参数初始化：应随机初始化权重参数矩阵$W$，而非将其全置为 0；
  - 若将各隐藏单元参数均初始化为 0，则所有隐藏单元均是对称的，均在计算完全相同的函数；
  - 若需要使得两个隐藏单元计算不同的函数，则应随机初始化；
  - 一般将参数初始化为较小的随机值，若将其初始化为较大的随机值，则有可能落入 Sigmoid 函数或 tanh 函数中导数接近于 0 的区域；
  - 注意：对于参数$b$，可将其全部初始化为 0；
- 令$W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{1}{n^{[l-1]}})$，$n^{[l-1]}$即是第$l-1$层神经元数量，同时也是第$l$层输入的特征数量；

# 4. 深层神经网络

## 4.1 前向传播与反向传播

- 在正向传播过程中，将计算所得的$W^{[l]}$、$b^{[l]}$、$z^{[l]}$缓存下来，供反向传播时使用；
- 在反向传播中，输入为$da^{[l]}$和在正向传播计算中得到的$z^{[l]}$，输出为$da^{[l-1]}$、$dW^{[l]}$、$db^{[l]}$，以供梯度下降算法使用，更新$W^{[l]}$和$b^{[l]}$；

## 4.2 核对矩阵维数

- 第$l$层权重参数矩阵$W^{[l]}$的维度应为$n^{[l]}\times n^{[l-1]}$；
  - 理解：将前一隐藏层的激活值向量转换为后一隐藏层的激活值向量；
  - 反向传播时，$dW^{[l]}$的维度与$W^{[l]}$相同；
- 第$l$层参数$b$的维度应为$n^{[l]}\times 1$；
  - 反向传播时，$db$的维度与$b$相同；

## 4.3 参数和超参数

- 参数：通过学习得到；
  - 权重参数$W$；
  - $b$；
- 超参数：超参数需要人为设置，而非学习得到，其将影响学得的参数结果；
  - 梯度下降法循环的次数；
  - 隐藏层数$L$；
  - 各隐藏层单元数；
  - 学习率$\alpha$；
  - 每个隐藏层的激活函数类型；
  - $\lambda$：正则化参数；
- 选取神经网络层数的方法：
  - 先尝试单层、两层的神经网络模型；
  - 将隐藏层层数作为一个超参数，在交叉验证集上评估；
- 选取超参数的方法：设计多组对照实验，观察代价函数在多次迭代过程中的变化规律，筛选最优的超参数；
  - 实践过程中，尽量不要自己设置超参数，而是查看文献中别人采用了哪些超参数；

# 5. 调参相关概念

## 5.1 数据集切分

- 数据集：
  - 训练集；
  - 交叉验证集：亦称开发集，选取最佳模型；
  - 各隐藏层单元数；
  - 测试集：评估被选定的最佳模型性能；
- 常见数据集切分方式：
  - 数据集规模较小：
    - 训练集：测试集=7:3；
    - 训练集：交叉验证集：测试集：6:2:2；
  - 数据集规模较大：
    - 交叉验证集和测试集的占比可低于 20%；
    - 对于百万规模以上的数据，可降低交叉验证集和测试集的数据占比，E.g. 98:1:1；
- 尽量使得训练集、交叉验证集和测试集中的数据服从相同的分布规律；

## 5.2 模型训练经验

- 贝叶斯误差：即最优误差，即各模型所能达到的理论上限；
  - 若模型误差低于贝叶斯误差，则可能出现了过拟合；
  - 贝叶斯误差不易求得，常使用人工完成该任务的误差近似替代贝叶斯误差；
- 可避免误差：模型误差与贝叶斯误差之间的差值；
- 若模型出现 high bias：即模型未能很好的拟合数据；
  - 增大网络规模：增加隐藏层、隐藏层单元的数量；
  - 延长训练时间；
  - 更换新的网络结构；
- 若模型出现 high variance：即过拟合；
  - 增加数据规模；
  - 正则化；
  - 更换新的网络结构；

## 5.3 $L2$正则化

- 逻辑回归中使用$L2$正则化，即使用$W$的$L2$范数作为正则化项；
  - $W$是一个高维参数矢量，包含大量的参数，可表达高偏差问题；
  - $b$仅为单个参数，即使对其做正则化，对高偏差问题影响有限；
- 正则化的作用：
  - 若正则化参数$\lambda$较大，则为使代价函数较小，应减小$W$中各元素的值；
  - 使得多个隐藏单元的值为 0 ，等价于缩小神经网络规模，极端情况下将使得网络等价于线性回归，此时网络无法表示过于复杂的函数，从而避免过拟合；

## 5.4 Dropout 正则化

- Dropout 正则化：随机失活；
  - 实现 Dropout 常见的方法：Inverted dropout，反向随机失活；
  - 遍历各层网络，设置保留神经网络中节点的概率 keep-prob  ；
  - 使用删减后的网络进行训练；
  - 通过除以 keep-prob 值，确保删除部分隐藏单元后，激活值的期望不变；
  - 测试阶段不使用 Dropout 函数，避免测试阶段输出的预测结果是随机的；
  - 对于有可能出现过拟合，且含有较多参数的层，可将 keep-prob 设置为较小值；
  - 对于输入、输出层，一般将 keep-prob 设置为 1.0；
  - 使用 Dropout 时，代价函数将不再单调递减；
    - 可先将各层 keep-prob 设置为 1.0，观察代价函数是否单调递减；
    - 然后应用 Dropout；
- Dropout 有效的直观解释：使得模型不依赖于某一个具体特征（即具体特征可能被随机失活），在学习过程中倾向于为各个特征均增加权重；

## 5.5 其他正则化方法

- Data augment：数据增强；
- Early stopping：提前中止模型训练；
  - 在模型训练过程中，绘制训练集误差和验证集误差，当验证集误差取较小值时，停止训练过程，避免过拟合；

## 5.6 输入归一化

- 输入归一化：可加速训练；
  - 设置均值为 0：令各样本值减去样本均值，使新的样本均值为 0；
  - 令方差为 1：令各样本除以$\sigma^2$，使方差归一化；
  - $\mu$和$\sigma^2$通过训练集计算得到；
  - 在训练集和测试集上，应使用相同的$\mu$和$\sigma^2$实现输入归一化；

## 5.7 梯度消失和梯度爆炸

- $W^{[l]}>I$：若权重矩阵略大于单位阵，则神经网络的激活函数将爆炸式增长；
  - 大量参数的值变为 NaN，或者不是数字的情况，意味网络计算出现了数值溢出；
  - 梯度修剪：用于解决梯度爆炸问题，观察梯度向量，若其大于某个阈值，缩放梯度向量，保证它不会太大；
- $W^{[l]}<I$：若权重矩阵略小于单位阵，则神经网络的激活函数将以指数级递减；

# 6. 优化算法

## 6.1 Mini-batch 梯度下降法

- 优化算法的作用：加快模型训练过程，便于快速找出最优模型；
- Mini-batch：
  - 符号：$X^{\{i\}}$，第$i$个 Mini-batch；
  - 将一个较大的训练集切分为若干个较小的训练集，称之为 Mini-batch；
  - 代价值关于 Mini-batch 次数的函数曲线整体向下，但存在一些噪声波动；
- 各种梯度下降方法：
  - 若 Mini-batch size 为样本总数，则为 batch gradient decant；
  - 若 Mini-batch size 为 1，则为随机梯度下降；
  - Mini-batch size 取值范围$[1, m]$；
- Mini-batch size 的选取原则：
  - 若样本总数较小，则 Mini-batch size 取 m，使用 batch gradient decant，E.g. 少于 2000 个样本；
  - 一般的 Mini-batch size 取 64，128，256，512；
- 1 epoch：遍历一次训练集，完成训练；

## 6.2 指数加权平均算法

- 指数加权平均计算公式：
  - $v_0=0$；
  - $v_1=0.9v_0+0.1\theta _1$；
  - $v_i=0.9v_{i-1}+0.1\theta _i$；
  - 注意：参数可记为$\beta$，$v_i=\beta v_{i-1}+(1-\beta)\theta _i$；
- 指数加权平均的偏差修正：
  - 由于$v_0=0$，因此指数加权平均算法在预测初期估计不准确；
  - 偏差修正后的公式：$v_i=\frac{\beta v_{i-1}+(1-\beta)\theta _i}{1-\beta ^i}$；
  - 分母中的$\beta^i$将随着$i$的增大而逐渐失效；
  - 实际使用梯度下降过程中，一般不使用偏差修正，因为移动平均已通过初始阶段；

## 6.3 Momentum 优化算法

- Momentum：动量梯度下降算法；
  - 优点：运行速度快于梯度下降算法；
  - 原理：
    - 消除梯度下降中的摆动；
    - 增大部分方向上的学习率，减小部分方向上的学习率，从而缩短梯度下降的时间开销；
  - 实现方式：
    - $\beta$：超参数，用于控制指数加权平均值，一般取 0.9；
    - $V_{dW}=\beta V_{dW}+(1-\beta)dW$；
    - $V_{db}=\beta V_{db}+(1-\beta)db$；
    - $W=W-\alpha V_{dW}$；
    - $b=b-\alpha V_{db}$；

## 6.4 RMSprop 优化算法

- RMSprop 优化算法：Root mean square prop 优化算法；
  - 消除梯度下降中的摆动；
  - $\beta$：超参数，用于控制指数加权平均值，一般取 0.999；
  - $S_{dW}=\beta S_{dW}+(1-\beta)(dW)^2$；
  - $S_{db}=\beta S_{db}+(1-\beta)(db)^2$；
  - $W:=W-\alpha \frac{dW}{\sqrt{S_{dW}}+\xi}$；
  - $b:=b-\alpha \frac{db}{\sqrt{S_{db}}+\xi}$；
  - 注意：$\xi$为一个较小常数，避免分母趋近于 0，一般取$\xi=10^{-8}$；

## 6.5 Adam 优化算法

- Adam 优化算法：将 Momentum、RMSprop 两种优化算法结合；
  - 注意：使用 Adam 优化算法时，需进行偏差修正；
  - 实现方式：
    - $V_{dW}=\beta _1 V_{dW}+(1-\beta _1)dW$；
    - $V_{db}=\beta _1 V_{db}+(1-\beta _1)db$；
    - $S_{dW}=\beta _2 S_{dW}+(1-\beta _2)(dW)^2$；
    - $S_{db}=\beta _2 S_{db}+(1-\beta _2)(db)^2$；
    - $V^{corrected}_{dW}=\frac{V_{dW}}{1-\beta ^t_1}$；
    - $V^{corrected}_{db}=\frac{V_{db}}{1-\beta ^t_1}$；
    - $S^{corrected}_{dW}=\frac{S_{dW}}{1-\beta ^t_2}$；
    - $S^{corrected}_{db}=\frac{S_{db}}{1-\beta ^t_2}$；
    - $W:=W-\alpha \frac{V^{corrected}_{dW}}{\sqrt{S^{corrected}_{dW}}+\xi}$；
    - $b:=b-\alpha \frac{V^{corrected}_{db}}{\sqrt{S^{corrected}_{db}}+\xi}$；

## 6.6 学习率衰减

- 超参数：
  - $a_0$：初始学习率；
  - $decayRate$：衰减率；
- 常见的学习率衰减方法：
  - $a=\frac{1}{1+deayRate*epochNum}a_0$；
  - epochNum：训练集遍历的次数，epoch 代数；

## 6.7 局部最优问题

- 当神经网络规模较大，参数较多时，一般不会困在局部最优解之中；

# 7. 超参数调试和 Batch 正则化

## 7.1 超参数调试

- 超参数：超参数需要人为设置，而非学习得到，其将影响学得的参数结果；
  - 学习率$\alpha$；
  - 各隐藏层单元数；
  - Mini-batch size；
  - 隐藏层数$L$；
  - 学习率衰减参数：decayRate，epochNum；
  - Adam 优化算法中的超参数：$\beta _1$，$\beta _2$，$\xi$（无需调试，一般取默认值）；
  - 每个隐藏层的激活函数类型；
  - 梯度下降法循环的次数；
  - $\lambda$：正则化参数；
- 选取超参数的方式：以两个超参数为例；
  - 在二维平面中随机选取若干个坐标点，取最优参数；
  - 缩小搜索范围，重复上述步骤；

## 7.2 Batch 归一化

- Batch 归一化：Batch Norm（BN），将归一化过程从输入层扩大到各个隐藏层；
- 归一化$Z^{[l]}$，便于更快训练$W^{[l+1]}$和$b^{[l+1]}$；

## 7.3 Softmax 回归

- 逻辑回归：二分类；
- Softmax 回归：多分类；
  - Hard max：将向量$Z^{[L]}$中最大元素的对应位置上输出 1， 其他位置输出 0；
  - 输出层单元数量等于类别数量；
  - E.g. 若输入为$X$，实现 3 分类问题，则输出层分别为$P(Other|X)$、$P(Class 1|X)$、$P(Class 2|X)$，输出概率之和为 1；
- Softmax 实现方式：
  - E.g. 若在第$L$层实现 Softmax，则有$Z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]}$；
  - 计算临时变量$t=e^{Z[L]}$；
  - 激活函数：$a^{[L]}=\frac{e^{Z[L]}}{\sum_{j=1}^4t_i}$；
  - 所得结果应表示各个类别上的条件概率，故令向量$Z^{[L]}$中的每个元素除以各元素之和$t$，使得概率之和为 1；
- 代价函数：
  - 单个样本的代价：$J(\widehat{y}, y)=-\sum_{j=1}^4y_jlog\widehat{y_j}$；
  - 所有样本的代价：$J(W^{[L]},b^{[L]})=\frac{1}{m}\sum_{i=1}^mJ(\widehat{y}^{(i)}, y^{(i)})$；

# 8. 机器学习策略

## 8.1 正交化

- 正交化：每次调整仅涉及某一个方面，不修改其他方面的内容；提升模型性能的方法：
  - 增加训练数据；
  - 收集多样化的数据；
  - 使用梯度下降算法训练模型时，延长其训练时间；
  - 尝试使用不同的梯度下降优化算法，E.g. Adam；
  - 正则化：Dropout、$L_2$；
  - 修改网络结构：
    - 使用更大或更小的网络；
    - 修改隐藏层单元数量；
    - 替换激活函数；
- 需要调整的四个步骤：
  - 训练集上的表现；
  - 交叉验证集上的表现；
  - 测试集上的表现；
  - 真实场景下的表现；

## 8.2评估模型性能的指标

- 尽量使用单一数值评估指标，便于比较各模型性能；
- 满足指标和优化指标：
  - 满足指标：E.g. 运行时间不超过 100 ms，只需满足要求即可；
  - 优化指标：E.g. 查准率越高越好；

## 8.3 误差分析

- 误差分析：统计交叉验证集中错误的样本的标签类型，及其占错误总数的比例；
  - 判断解决各个问题对模型性能的提升空间；
  - 优先处理影响较大的问题；

## 8.4 迁移学习

- 迁移学习：
  - 预训练：在原领域数据集上训练网络参数；
  - 微调：
    - 若新领域的数据集规模较小，则保持原网络中前$L-1$层参数不变，仅在新领域数据集上训练最后一层的参数；
    - 若新领域的数据集规模较大，则可在新领域数据集上训练最后多层的参数；
- 迁移学习适用场景：任务 A 中的低层次特征对于任务 B 有帮助；

## 8.5 多任务学习

-  多任务学习适用场景：
  - 任务 A 中的低层次特征对于任务 B 有帮助；
  - 每个任务的数量相近；
  - 可以训练一个较大的网络同时完成多个任务；

## 8.6 端到端深度学习

- 端到端深度学习的优点：
  - 借助神经网络直接学习数据特征，避免被迫引入人类的成见；
  - 减少需要手工设计的组件数量；
- 端到端深度学习的缺点：
  - 需要大量的数据；
  - 排除了可能有用的手工设计的组件；

# 9. 循环序列模型

## 9.1 符号

- $T^{(i)}_x$：第$i$个训练样本的输入序列长度；
- $T_y$：输出序列的长度；
- $x^{<t>}$：输入的时序序列中第$t$个元素；
- $y^{<t>}$：输出的时序序列中第$t$个元素；
- $X^{(i)<t>}$：第$i$个训练样本中第$t$个元素；

## 9.2 RNN

- 为什么不使用标准的神经网络处理时序数据？
  - 不同样本的输入输出序列长度不同；
  - 如果简单的填充 0 将其补充至最大长度，不是一种很好的表示方式；
  - 无法共享从不同位置上学到的特征；
  - 适当的表示形式能减少参数数量；
- RNN 的参数：
  - 每个时间步的参数共享；
    - $W_{ax}$：从$x^{<i>}$到隐藏层的连接的参数；
    - $W_{aa}$：用于计算激活值；
    - $W_{ya}$：决定输出结果；
  - 计算公式：
    - $a^{<t>}=g_1(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)$；
    - $\widehat{y}^{<t>}=g_2(W_{ya}a^{<t>}+b_y)$；
  - 通常使用零向量作为输入，即零时刻的激活值$a^{<0>}$；
  - 激活函数：常用 tanh；

## 9.3 不同类型的 RNN

- RNN：可将$y^{<t-1>}$作为$x^{<t>}$；
  - 多对对：输入序列的长度等于输出序列的长度；
  - 多对多：输入序列的长度不等于输出序列的长度；
  - 多对一、一对多……
- EOS：End of sentence，句子的结尾；
- 对新序列采样：
  - 在模型训练完成后，如果需要了解模型学到了什么，则对其进行一次新序列采样；
  - 令$a^{<0>}=0$，$x^{<1>}=0$，$x^{<t>}=y^{t-1}$，即可得到一个输出序列，用于分析模型的学习效果；

## 9.4 RNN 的梯度消失问题

- 标准的 RNN 模型不擅长捕获长期依赖关系；
  - 当网络层数过深时，由于梯度消失，在反向传播过程中，后面后面层的输出误差很难影响前面层的计算；

## 9.5 GRU

- GRU：改变 RNN 的隐藏层，能更好地捕捉深层连接，并改善梯度消失问题；
  - $\widetilde{c}^{<t>}$是$c^{<t>}$的候选值；
  - 更新门决定何时更新$c^{<t>}$；
  - $c^{<t>}=\Gamma _u*\widetilde{c}^{<t>}+(1-\Gamma_u)*c^{<t-1>}$；
- c：Memory cell，记忆细胞；
  - $c^{<t>}=a^{<t>}$；
  - 在每个时间步，用一个候选值重写记忆细胞$\widetilde{c}^{<t>}$的值，所以它就是个候选值，替代$c^{<t>}$；
  - $\widetilde{c}^{<t>}=tanh(W_c[\Gamma _r*c^{<t-1>},x^{<t>}]+b_c)$；
- 更新门：$ \Gamma _u$；
  - $ \Gamma _u=\sigma (W_u[c^{<t-1>},x^{<t>}]+b_u)$；
  - 值域$(0,1)$；
- 表示相关性的门：$\Gamma _r$；
  - $\Gamma _r=\sigma (W_r[c^{<t-1>},x^{<t>}]+b_r)$；

## 9.6 LSTM

- LSTM：
  - $\widetilde{c}^{<t>}=tanh(W_c[a^{<t-1>},x^{<t>}]+b_c)$；
  - $c^{<t>}=\Gamma _u*\widetilde{c}^{<t>}+\Gamma_f*c^{<t-1>}$；
  - $a^{<t>}=\Gamma _o*tanh\space c^{<t>}$；
  - 更新门：$ \Gamma _u=\sigma (W_u[a^{<t-1>},x^{<t>}]+b_u)$；
  - 遗忘门：$ \Gamma _f=\sigma (W_f[a^{<t-1>},x^{<t>}]+b_f)$；
  - 输出门：$ \Gamma _o=\sigma (W_o[a^{<t-1>},x^{<t>}]+b_o)$；

## 9.7 双向 RNN 和深层 RNN

- $\widehat{y}^{<t>}=g(W_g[\overrightarrow{a}^{<t>},\overleftarrow{a}^{<t>}]+b_y)$；
- 深层 RNN：将多层 RNN 纵向叠加；

# 10. NLP 和词嵌入

## 10.1 词的表示方法

- 词的表示方法：
  - one-hot 表示法；
  - 词嵌入；
- one-hot 表示法：构建一个与词典大小相同的向量，某个词对应位置为 1，其余位置值为 0；
  - 对于词典中不包含的词，可将其替换为 UNK，表示未知词，即对于 UNK 建立概率模型，而不针对各个未知词建立模型；
  - 任意两个向量的内积为零，无法表示词之间的关联关系；
- 词嵌入：
  - 获取方法：
    - 从大量的文本中学习词嵌入模型；
    - 从网上下载预训练后的词嵌入模型；
    - 将获取到的词嵌入模型迁移到自己的任务中；
  - 词嵌入所的向量维度低于 one-hot 所的向量；

## 10.2 词嵌入算法

- Skip-grams 模型：一种 Word2Vec 算法，抽取上下文和目标词配对，用于获取词嵌入模型；
- 负采样；
- GloVe 词向量；

# 11. 序列模型和注意力机制

## 11.1 序列模型

- seq2seq 模型：Sequence to sequence 模型；
  - 编码网络；
  - 解码网络；
- 为什么不使用贪心策略：
  - 贪心策略将逐个输出概率最大的词，而用户更希望能使得语句整体出现的概率最大化；
  - $arg\space maxP(\widehat{y}^{<1>},\widehat{y}^{<2>},\cdots,\widehat{y}^{<T_y>})|x$；

![](.\Picture\seq2seq.JPG)

- 集束搜索：一种近似搜索算法，亦称启发式算法，不保证能找出可能性最大的句子，被用于优化学习算法（E.g. seq2seq/RNN）输出的目标函数；
  - 集束宽：beam width，根据需要设置，即每次考虑 beam width 个可能的后继词；
  - 相较于贪心策略（beam width），集束搜索的结果通常会更好；
  - 实现$arg\space \underset{y}{max}\prod_{t=1}^{T_y} \space P(y^{<t>}|x,y^{<1>},\cdots,y^{<t-1>})$；
- 改进集束搜索：使用长度归一化；
  - 实现：$\frac{1}{T^{\alpha}_y}\sum_{y=1}^{T_y} log\space P(y^{<t>}|x,y^{<1>},\cdots,y^{<t-1>})$；
    - ${T_y}$：输出语句所含词的数量；
    - $\alpha$：超参数，一般取 0.7；
      - 若$\alpha=1$，则除以句子中所含词的总数；
      - 若$\alpha=0$，则未进行归一化；
  - 由于各个概率值均小于 1，原式更倾向于输出较短的语句；改进后的集束搜索致力于实现长度归一化后的概率最大，从而避免惩罚长语句；
  - 由于集束搜索中各个概率累乘，所的结果为一个极小的值，在计算机中无法精确存储；改进后的集束搜索将原式取对数，使得数值更稳定，不容易出现四舍五入的误差；

## 11.2 Bleu 得分

- Bleu 得分：
  - 当模型可能输出多个较好的预测结果，使用 Bleu 判断准确性；
  - 若模型的预测结果接近于任意一个人工结果时，将得到较高的 Bleu 得分；
  - 实现方式：预测结果中的词组在标准答案中出现的频率；

## 11.3 Attention 模型

- 在神经网络中，记忆非常长的句子较为困难；

# 12. 卷积神经网络

## 12.1 边缘检测

- 核/过滤器：
  - 符号：$*$；
  - 表示卷积运算；
  - 过滤器的大小一般是$奇数\times 奇数$；
  - 过滤器的取值：可将过滤器中的值作为参数，通过反向传播学习得到；
- 和只用全连接层相比，卷积层的优势在于：
  - 参数共享：参数仅与过滤器有关，与图像大小无关；
  - 稀疏连接：输出结果中的某个像素值，仅与原图像中部分像素点有关；
  - 上述两个原因导致卷积层参数较少，从而可以使用较小规模的数据集训练模型，而不会出现过拟合；

## 12.2 Padding

- 卷积的缺点：
  - 若原图像尺寸为$n\times n$，过滤器大小为$f\times f$，步长为 1，则输出结果的尺寸为$n-f+1$；
  - 卷积操作的结果小于原图像，多次卷积操作将使得图像不断减小，不利于构建深度神经网络；
  - 不同位置的像素参与计算的次数不同，将丢失边缘信息；
- Padding：
  - 在图像外用 0 填充$p=\frac{f-1}{2}$层像素点，然后执行卷积操作，输出图像尺寸将等于原图尺寸；
  - 充分利用边缘像素点的信息；

## 13.3 卷积步长

- 若原图像尺寸为$n\times n$，过滤器大小为$f\times f$，步长为$s$，则输出结果的尺寸为$(\frac{n+2p-f}{s}+1)\times (\frac{n+2p-f}{s}+1)$；
  - 若$(\frac{n+2p-f}{s}+1)$不为整数，则向下取整；

## 13.4 三维卷积

- 三维卷积：过滤器与原图像中对应位置上的像素点值的乘积之和；
  - height：原图像的高；
  - weight：原图像的宽；
  - channels：通道数，对于彩色 RGB 图像，其$channels=3$；
    - 三维卷积操作中，过滤器的通道数应与原图像中的通道数相等；
  - 三维卷积输出结果的大小：
    - height、weight 的大小与二维卷积中计算方式相同；
    - 输出结果的大小为$height\times weight\times 1$；

## 13.5 单层 CNN

- $f^{[l]}$：第$l$层的过滤器大小；
- $p^{[l]}$：第$l$层的 Padding 大小；
- $s^{[l]}$：第$s$层卷积操作的步长；
- $n^{[l]}_c$：第$l$层的过滤器数量；
- CONV：卷积层，convolution；
- POOL：池化层，Pooling；
- FC：全连接层，Fully connected；

## 13.6 池化层和全连接层

- 池化层：缩减图像的大小，提高计算速度，提高所提取特征的鲁棒性；
  - 最大池化：取过滤器对应的原图像区域内的最大值；
  - 平均池化；
  - 池化过程中没有需要学习的参数，只有过滤器大小$f$、步长$s$等超参数；
  - E.g. 若$f=2$，$s=2$，则输出图像的长、宽均减半；
- 在计算神经网络的层数时，通常只统计具有权重和参数的层。因为池化层没有权重和参数，只有一些超参数，因此统计网络层数时，将卷积层和池化层视为 1 层；
  - 也有论文中将其视为 2 层；
- 全连接层：即当前层中的各个隐藏单元与前一层中的各个隐藏单元连接；
- 常见的网络结构：若干个卷积层-池化层、若干个全连接层、Softmax；

# 13. CNN 实例学习

## 13.1 经典的网络结构

- 经典的网络结构：
  - LeNet-5；
  - AlexNet；
  - VGG：
    - VGG-16：卷积层、池化层共有 16 层；
    - VGG-19：卷积层、池化层共有 19 层；

## 13.2 残差网络

- 非常深的网络存在梯度消失、梯度爆炸现象，因此难以训练；
- 跳跃连接：可从某一层网络层获取激活值，然后迅速反馈给另外一层，甚至是神经网络的更深层；
- ResNet：残差网络，由残差块构成，使用残差块有助于解决梯度消失和梯度爆炸问题，能训练更深的神经网络；
  - 152 层；
  - 未使用残差块的网络，随着网络层数的增加，训练误差先减小，而后增大；
  - 使用残差块的网络，随着网络层数的增加，训练误差持续减小；
- 残差块：
  - short cut/skip connection：捷径/跳远连接；
  - 将某层的激活值跳过若干层，叠加到路径中；

## 13.3 Inception

- Inception；

# Unsolved

- 表征学习：是机器学习中的一类方法，用于自动找出适合表示数据的方式，而深度学习是具有多级表示的表征学习方法，可以逐级表示越来越抽象的概念或模式；

# TODO

- 10月31日前完成 Course 4；
- 11月19日前完成 CS 224n；
- 择机完成李宏毅 NLP 课程《深度学习与人类语言处理》，见 B 站；
- 当前进度：Course 4，Week 2，Chapter 3 已完成；
  - 每周学习 4 Weeks 的课程，即每天学习 1 Week，剩余 3 天学习纸质教材；
- 在理论学习期间择机阅读：
  - 《深度学习框架 PyTorch 入门与实践》；
  - 《自然语言处理入门》；
  - 《动手学深度学习》；
- 《动手学深度学习》进度：	
  - 总页数：390 页；
  - 每天 10 页，8 月 30 日完成；
  - 当前进度：Page 24，Chapter 1 已完成；

# References

[1] Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola. Dive into Deep Learning[M]. 人民邮电出版社, 2019. 
[2] https://www.coursera.org/specializations/deep-learning?