[toc]

# 版权声明

- 深度学习系列学习笔记来源于：
  - Aston Zhang，Zack C. Lipton，李沐和 Alex J. Smola 所著 *Dive into Deep Learning* [1]；
  - Andrew Ng 教授在 Coursera 网站上所授课程《Deep Learning》[2]；；
- 该系列笔记不以盈利为目的，仅用于个人学习、课后复习及科学研究；
- 如有侵权，请与本人联系（hqpan@foxmail.com），经核实后即刻删除；
- 本文采用 [署名-非商业性使用-禁止演绎 4.0 国际 (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh) 协议发布；

# 1. 深度学习概论

## 1.1 基本概念

- 隐藏单元：hidden units；
- CNN 和 RNN：
  - CNN：常用于处理图像数据；
  - RNN：常用于处理一维时序数据；
- 结构化数据和非结构化数据：
  - 结构化数据：E.g. 数据库中的一张表；
  - 非结构化数据：文本、音频、图像；
- 影响最终性能的因素：
  - 当训练集规模较小时，最终性能取决于开发人员手工设计特征的能力（hand engineer features）及算法处理方面的细节，此时传统机器学习方法的性能可能会超过神经网络；
  - 当训练集规模较大时，最终性能取决于算法模型自身，此时大规模神经网络的性能远胜于传统机器学习方法；

## 1.2 符号含义

- $a^{[i]}$：第$i$层中的激活值向量；
  - $a^{[0]}$即为输入特征$x$；
  - $a^{[L]}=\widehat{y}$；
  - $a^{[i]}_j$：第$i$层中的激活值向量，其中的第$j$个元素取值；
- $a^{[l]}$：第$l$层中的激活函数值；
- $g^{[l]}(x)$：第$l$层中选取的激活函数；
- L：神经网络层数；
- m：训练样本数量，即数据集规模；
  - $m_{train}$：训练集样本数；
  - $m_{test}$：测试集样本数；
- $n^{[l]}$：第$l$层中隐藏单元的数量；
- $n_x$：特征向量$x$的维度，有时简写为$n$；
- W、b：
  - W：权重参数；
  - b：阈值；
- $(x,y)$：表示一个样本；
- $(x^{(i)},y^{(i)})$：表示数据集中的第$i$个样本；
- $\widehat{y}$：模型输出的对$y$的预测值；

# 2. 神经网络基础

- **计算图**的计算求解：
  - 前向传播；
  - 反向传播；
- 向量化相较于循环语句，计算速度快；
- Python 中的广播机制；

# 3. 浅层神经网络

## 3.1 神经网络的表示

- 神经网络的表示：统计神经网络层数时，不统计输入层，仅统计隐含层和输出层；
  - 输入层；
  - 隐藏层：可为不同的隐藏层选择不同的激活函数；
  - 输出层；
- 激活值：输入特征的数值，网络中不同层的值；

## 3.2 激活函数

- 常见的非线性激活函数：
  - Sigmoid 函数；
  - tanh 函数；
  - ReLU 函数；
  - the Leaky ReLU 函数；
- Q：为什么神经网络中需要使用非线性激活函数？
  - A：若使用线性激活函数，E.g. $y=x$，则$\widehat{y}$将是所有输入特征的线性组合，性能等价于没有任何隐藏层的网络，即线性隐藏层没有价值；
  - 注意：若处理线性回归问题，$y$是一个实数，则输出层可使用线性激活函数；
- Sigmoid 函数：$\sigma(x)=\frac{1}{1+e^{-x}}$，$x\in(-\infty,+\infty)$；
  - 功能：将$(-\infty,+\infty)$之间的值映射到区间$(0,1)$上；
  - 特点：在$-\infty$和$+\infty$方向上梯度为 0；
  - 使用梯度下降法时，若梯度接近于 0，参数更新将会减缓；
  - 注意：Sigmoid 函数适用于二分类的输出层；
- tanh 函数：双曲正切函数；
  - $tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}$；
  - 值域为$(-1,1)$；
  - 使用梯度下降法时，若梯度接近于 0，参数更新将会减缓；
  - 注意：在大多数情况下，tanh 函数的性能优于 Sigmoid 函数，前者的平均值更接近于 0；
- ReLU函数：Rectified Linear Unit，线性修正单元；
  - $y=max(0,x)$；
  - 修正：指其取值不小于 0；
  - 对于大于 0 的输入，其梯度始终为 1；
  - 将 sigmoid 函数替换为 ReLU 函数，将使梯度下降算法运行得更快，神经网络学习速度更快；
- the Leaky ReLU 函数：带泄露的 ReLU 函数；
  - ReLU 函数的缺陷：当$x<0$时，函数的导数为 0；
  - 当$x<0$时，the Leaky ReLU 函数是一个非常接近于 0 的正值；
- 选取激活函数的经验法则：
  - Sigmoid 函数适用于二分类的输出层；
  - 在其他情况下，对于其它各隐藏层，应使用 ReLU 函数；

## 3.3 神经网络参数初始化

- 神经网络参数初始化：应随机初始化权重参数矩阵$W$，而非将其全置为 0；
  - 若将各隐藏单元参数均初始化为 0，则所有隐藏单元均是对称的，均在计算完全相同的函数；
  - 若需要使得两个隐藏单元计算不同的函数，则应随机初始化；
  - 一般将参数初始化为较小的随机值，若将其初始化为较大的随机值，则有可能落入 Sigmoid 函数或 tanh 函数中导数接近于 0 的区域；
  - 注意：对于参数$b$，可将其全部初始化为 0；
- 令$W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{1}{n^{[l-1]}})$，$n^{[l-1]}$即是第$l-1$层神经元数量，同时也是第$l$层输入的特征数量；

# 4. 深层神经网络

## 4.1 前向传播与反向传播

- 在正向传播过程中，将计算所得的$W^{[l]}$、$b^{[l]}$、$z^{[l]}$缓存下来，供反向传播时使用；
- 在反向传播中，输入为$da^{[l]}$和在正向传播计算中得到的$z^{[l]}$，输出为$da^{[l-1]}$、$dW^{[l]}$、$db^{[l]}$，以供梯度下降算法使用，更新$W^{[l]}$和$b^{[l]}$；

## 4.2 核对矩阵维数

- 第$l$层权重参数矩阵$W^{[l]}$的维度应为$n^{[l]}\times n^{[l-1]}$；
  - 理解：将前一隐藏层的激活值向量转换为后一隐藏层的激活值向量；
  - 反向传播时，$dW^{[l]}$的维度与$W^{[l]}$相同；
- 第$l$层参数$b$的维度应为$n^{[l]}\times 1$；
  - 反向传播时，$db$的维度与$b$相同；

## 4.3 参数和超参数

- 参数：通过学习得到；
  - 权重参数$W$；
  - $b$；
- 超参数：超参数需要人为设置，而非学习得到，其将影响学得的参数结果；
  - 梯度下降法循环的次数；
  - 隐藏层数$L$；
  - 各隐藏层单元数；
  - 学习率$\alpha$；
  - 每个隐藏层的激活函数类型；
  - $\lambda$：正则化参数；
- 选取神经网络层数的方法：
  - 先尝试单层、两层的神经网络模型；
  - 将隐藏层层数作为一个超参数，在交叉验证集上评估；
- 选取超参数的方法：设计多组对照实验，观察代价函数在多次迭代过程中的变化规律，筛选最优的超参数；

# 5. 调参相关概念

## 5.1 数据集切分

- 数据集：
  - 训练集；
  - 交叉验证集：选取最佳模型；
  - 各隐藏层单元数；
  - 测试集：评估被选定的最佳模型性能；
- 常见数据集切分方式：
  - 数据集规模较小：
    - 训练集：测试集=7:3；
    - 训练集：交叉验证集：测试集：6:2:2；
  - 数据集规模较大：
    - 交叉验证集和测试集的占比可低于 20%；
    - 对于百万规模以上的数据，可降低交叉验证集和测试集的数据占比，E.g. 98:1:1；
- 尽量使得训练集、交叉验证集和测试集中的数据服从相同的分布规律；

## 5.2 模型训练经验

- 贝叶斯误差：即最优误差，即各模型所能达到的理论上限；
- 若模型出现 high bias：即模型未能很好的拟合数据；
  - 增大网络规模：增加隐藏层、隐藏层单元的数量；
  - 延长训练时间；
  - 更换新的网络结构；
- 若模型出现 high variance：即过拟合；
  - 增加数据规模；
  - 正则化；
  - 更换新的网络结构；

## 5.3 $L2$正则化

- 逻辑回归中使用$L2$正则化，即使用$W$的$L2$范数作为正则化项；
  - $W$是一个高维参数矢量，包含大量的参数，可表达高偏差问题；
  - $b$仅为单个参数，即使对其做正则化，对高偏差问题影响有限；
- 正则化的作用：
  - 若正则化参数$\lambda$较大，则为使代价函数较小，应减小$W$中各元素的值；
  - 使得多个隐藏单元的值为 0 ，等价于缩小神经网络规模，极端情况下将使得网络等价于线性回归，此时网络无法表示过于复杂的函数，从而避免过拟合；

## 5.4 Dropout 正则化

- Dropout 正则化：随机失活；
  - 实现 Dropout 常见的方法：Inverted dropout，反向随机失活；
  - 遍历各层网络，设置保留神经网络中节点的概率 keep-prob  ；
  - 使用删减后的网络进行训练；
  - 通过除以 keep-prob 值，确保删除部分隐藏单元后，激活值的期望不变；
  - 测试阶段不使用 Dropout 函数，避免测试阶段输出的预测结果是随机的；
  - 对于有可能出现过拟合，且含有较多参数的层，可将 keep-prob 设置为较小值；
  - 对于输入、输出层，一般将 keep-prob 设置为 1.0；
  - 使用 Dropout 时，代价函数将不再单调递减；
    - 可先将各层 keep-prob 设置为 1.0，观察代价函数是否单调递减；
    - 然后应用 Dropout；
- Dropout 有效的直观解释：使得模型不依赖于某一个具体特征（即具体特征可能被随机失活），在学习过程中倾向于为各个特征均增加权重；

## 5.5 其他正则化方法

- Data augment：数据增强；
- Early stopping：提前中止模型训练；
  - 在模型训练过程中，绘制训练集误差和验证集误差，当验证集误差取较小值时，停止训练过程，避免过拟合；

## 5.6 输入归一化

- 输入归一化：可加速训练；
  - 设置均值为 0：令各样本值减去样本均值，使新的样本均值为 0；
  - 令方差为 1：令各样本除以$\sigma^2$，使方差归一化；
  - $\mu$和$\sigma^2$通过训练集计算得到；
  - 在训练集和测试集上，应使用相同的$\mu$和$\sigma^2$实现输入归一化；

## 5.7 梯度消失和梯度爆炸

- $W^{[l]}>I$：若权重矩阵略大于单位阵，则神经网络的激活函数将爆炸式增长；
- $W^{[l]}<I$：若权重矩阵略小于单位阵，则神经网络的激活函数将以指数级递减；

# 6. 优化算法

## 6.1 Mini-batch 梯度下降法

- 优化算法的作用：加快模型训练过程，便于快速找出最优模型；
- Mini-batch：
  - 符号：$X^{\{i\}}$，第$i$个 Mini-batch；
  - 将一个较大的训练集切分为若干个较小的训练集，称之为 Mini-batch；
  - 代价值关于 Mini-batch 次数的函数曲线整体向下，但存在一些噪声波动；
- 各种梯度下降方法：
  - 若 Mini-batch size 为样本总数，则为 batch gradient decant；
  - 若 Mini-batch size 为 1，则为随机梯度下降；
  - Mini-batch size 取值范围$[1, m]$；
- Mini-batch size 的选取原则：
  - 若样本总数较小，则 Mini-batch size 取 m，使用 batch gradient decant，E.g. 少于 2000 个样本；
  - 一般的 Mini-batch size 取 64，128，256，512；
- 1 epoch：遍历一次训练集，完成训练；

## 6.2 指数加权平均算法

- 指数加权平均计算公式：
  - $v_0=0$；
  - $v_1=0.9v_0+0.1\theta _1$；
  - $v_i=0.9v_{i-1}+0.1\theta _i$；
  - 注意：参数可记为$\beta$，$v_i=\beta v_{i-1}+(1-\beta)\theta _i$；
- ~~指数加权平均的偏差修正~~：
  - 由于$v_0=0$，因此指数加权平均算法在预测初期估计不准确；
  - 偏差修正后的公式：$v_i=\frac{\beta v_{i-1}+(1-\beta)\theta _i}{1-\beta ^i}$；
  - 分母中的$\beta^i$将随着$i$的增大而逐渐失效；
  - 实际使用梯度下降过程中，一般不使用偏差修正，因为移动平均已通过初始阶段；

## 6.3 动量梯度下降算法

- 动量梯度下降算法：
  - 优点：运行速度快于梯度下降算法；
  - 原理：
    - 消除梯度下降中的摆动；
    - 增大部分方向上的学习率，减小部分方向上的学习率，从而缩短梯度下降的时间开销；
  - 实现方式：
    - $\beta$：超参数，用于控制指数加权平均值，一般取 0.9；
    - $v_{dW}=\beta v_{dW}+(1-\beta)dW$；
    - $v_{db}=\beta v_{db}+(1-\beta)db$；
    - $W=W-\alpha V_{dW}$；
    - $b=b-\alpha v_{db}$；

## 6.4 RMSprop

- RMSprop：Root mean square prop 算法；
  - 消除梯度下降中的摆动；
  - $S_{dW}=\beta S_{dW}+(1-\beta)(dW)^2$；
  - $S_{db}=\beta S_{db}+(1-\beta)(db)^2$；
  - $W:=W-\alpha \frac{dW}{\sqrt{S_{dW}}}$；
  - $b:=b-\alpha \frac{db}{\sqrt{S_{db}}}$；

## 6.5 Adam 算法

- 

# Unsolved

- 表征学习：是机器学习中的一类方法，用于自动找出适合表示数据的方式，而深度学习是具有多级表示的表征学习方法，可以逐级表示越来越抽象的概念或模式；

# TODO

- 10月18日前完成 Course 2 & 3；
- 10月25日前完成 Course 4；
- 10月31日前完成 Course 5；
- 11月19日前完成 CS 224n；
- 择机完成李宏毅老师的NLP视频课程；
- 当前进度：Course 2，Week 1，Chapter 5 已完成；
  - Course 2-5 共计 12 Weeks；
  - 每周学习 4 Weeks 的课程，即每天学习 1 Week，剩余 3 天学习纸质教材；
- 在理论学习期间择机阅读：
  - 《深度学习框架 PyTorch 入门与实践》；
  - 《自然语言处理入门》；
  - 《动手学深度学习》；
- 《动手学深度学习》进度：	
  - 总页数：390 页；
  - 每天 10 页，8 月 30 日完成；
  - 当前进度：Page 24，Chapter 1 已完成；

# References

[1] Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola. Dive into Deep Learning[M]. 人民邮电出版社, 2019. 
[2] https://www.coursera.org/specializations/deep-learning?