[toc]



# 版权声明

- 深度学习系列学习笔记来源于 Aston Zhang，Zack C. Lipton，李沐和 Alex J. Smola 所著 *Dive into Deep Learning* [1]；
- 该系列笔记不以盈利为目的，仅用于个人学习、课后复习及科学研究；
- 如有侵权，请与本人联系（hqpan@foxmail.com），经核实后即刻删除；
- 本文采用 [署名-非商业性使用-禁止演绎 4.0 国际 (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh) 协议发布；

# 1. 深度学习简介

## 1.1 定义

- 表征学习：是机器学习中的一类方法，用于自动找出适合表示数据的方式，而深度学习是具有多级表示的表征学习方法，可以逐级表示越来越抽象的概念或模式；
- $\mathcal{B}$：mini-batch SGD 中，每个小批量中的样本数量，即 batch size；
- $\eta$：学习率；

## 1.2 数据操作

- `torch.Tensor`：用于存储和变换数据，类似于多维数组，支持GPU加速；
  - tensor：n. 张量，可视为一个多维数组；
  - 标量：0维张量；
  - 向量：1维张量；
  - 矩阵：2维张量；
- 内存共享的数据操作：
  - 索引操作；
  - `view()`：若某个输入维度为-1，则自动计算该维度大小；
  - `torch.add(x, y, out = y)`；
  - `y += x`；
  - `y.add_(x)`：函数名中带下划线的函数，会修改Tensor本身，不带下划线的函数将返回一个新的Tensor；
- 非内存共享的操作：
  - `clone()`：还有⼀个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor；
  - `y = x + y`；
- Tensor 和 numpy 相互转换：这两个函数所产⽣的的 Tensor 和NumPy 中的数组共享内存；
  - `numpy()`；
  - `from_numpy()`；
  - 注意：
    - `torch.tensor()`可将 NumPy 中的 array 转换为 Tensor，且不共享内存；
    - 所有在CPU上的 Tensor （除了 CharTensor ）都⽀持与NumPy数组相互转换；
- ⽅法 to() ：可将 Tensor 在CPU和GPU（需要硬件⽀持）之间相互移动；
- `.cuda()`方法：将Tensor转换为GPU的Tensor，以便使用GPU加速；
- 3 种加法：
  - x + y；
  - t.add(x, y)；
  - t.add(x, y, out = result)；
- 函数名以`_`结尾的函数：均为 inplace 方式，即会修改原始数据；

## 1.3 自动求梯度

- grad 在反向传播过程中是累加的，每次反向传播过程中都会累加此前的梯度，因此反向传播之前需将梯度清零；
- Variable：
  - 简单封装了Tensor，支持几乎所有的Tensor操作，`.backward()`方法可实现反向传播；
  - 与Tensor接口几乎一致，可等同使用；

# 2. 深度学习基础

## 2.1 自动求梯度

- 在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传⼊任何参数；否则，需要传⼊⼀个与 y 同形的 Tensor；
- grad在反向传播过程中是累加的(accumulated)，即每⼀次运⾏反向传播，梯度都会累加之前的梯度，所以⼀般在反向传播之前需把梯度清零；
- 如果想要修改 tensor 的数值，但是⼜不希望被 autograd 记录（即不会影响反向传播），那么可对 tensor.data 进⾏操作；

## 2.2 线性回归

- 线性回归：
  - 假设输出与各个输⼊之间是线性关系；
  - 既可以⽤神经⽹络图表示线性回归，⼜可以⽤⽮量计算表示该模型；
  - 尽可能采⽤⽮量计算，以提升计算效率；
  - 作为⼀个单层神经⽹络，线性回归输出层中的神经元和输⼊层中各个输⼊完全连接。因此，线性回归的输出层⼜叫全连接层；
- 损失函数：计算预测值与真实值之间差值的函数，是以模型参数为参数的函数；
- 超参数：
  - 人为设定的 参数，而非通过模型训练学到的参数；
  - 调参：一般指反复试错调节超参数；
- 解析解和数值解：
  - 解析解：若模型和损失函数形式较为简单时，其误差最⼩化问题的解可⽤公式表示；
  - 数值解：大多数深度学习模型没有解析解，只能通过优化算法有限次迭代模型参数尽可能降低损失函数的值；
- ⼩批量随机梯度下降（mini-batch stochastic gradient descent）：mini-batch SGD；
  - 用途：一种求数值解的优化算法；
  - 实现方式：先选取⼀组模型参数的初始值，如随机选取；接下来对参数进⾏多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的⼩批量（mini-batch） ，然后求⼩批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积作为模型参数在本次迭代的减⼩量；
- 特征数：亦称特征数、特征向量维度，即神经网络中第一层输入值的个数；
- 输出层不涉及计算，因此计算神经网络层数时不统计输出层；
  - 线性回归可视为一个单层神经网络；
- 全连接层/稠密层：全连接层的每一个结点都与上一层的所有结点相连，将前边提取到的特征综合起来；
- 在一个周期（epoch）中，使用训练数据集中的所有样本各一次；

## 2.3 线性回归的实现方式

- 



# ==待整理内容==

- 链式法则：即反向传播，用于更新网络参数；

# TODO

- ==旧版本进度==：
  - 总页数 220 = 198 + 20：Chapter 1-6，9；
  - 当前进度：Page 80；
  - 每天 20 页，10 月 23 日完成；
- 学习计划：
  - 总页数：364 页；
  - 每天 10 页，01月 27日完成；
- 当前进度：
  - 当前进度：Page 58， 3.3 已完成；
  - 每天10页，12月24日-01月27日；

# References

[1] Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola. Dive into Deep Learning[M]. 人民邮电出版社, 2019. 