[toc]



# 版权声明

- 深度学习系列学习笔记来源于 Aston Zhang，Zack C. Lipton，李沐和 Alex J. Smola 所著 *Dive into Deep Learning* [1]；
- 该系列笔记不以盈利为目的，仅用于个人学习、课后复习及科学研究；
- 如有侵权，请与本人联系（hqpan@foxmail.com），经核实后即刻删除；
- 本文采用 [署名-非商业性使用-禁止演绎 4.0 国际 (CC BY-NC-ND 4.0)](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh) 协议发布；

# 1. 深度学习简介

## 1.1 定义

- 表征学习：是机器学习中的一类方法，用于自动找出适合表示数据的方式，而深度学习是具有多级表示的表征学习方法，可以逐级表示越来越抽象的概念或模式；
- 元素/单元：即数组或矩阵中的某个数值；
- $\mathcal{B}$：mini-batch SGD 中，每个小批量中的样本数量，即 batch size；
- $\eta$：学习率；

## 1.2 数据操作

- `torch.Tensor`：用于存储和变换数据，类似于多维数组，支持GPU加速；
  - tensor：n. 张量，可视为一个多维数组；
  - 标量：0维张量；
  - 向量：1维张量；
  - 矩阵：2维张量；
- 内存共享的数据操作：
  - 索引操作；
  - `view()`：若某个输入维度为-1，则自动计算该维度大小；
  - `torch.add(x, y, out = y)`；
  - `y += x`；
  - `y.add_(x)`：函数名中带下划线的函数，会修改Tensor本身，不带下划线的函数将返回一个新的Tensor；
- 非内存共享的操作：
  - `clone()`：还有⼀个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor；
  - `y = x + y`；
- Tensor 和 numpy 相互转换：这两个函数所产⽣的的 Tensor 和NumPy 中的数组共享内存；
  - `numpy()`；
  - `from_numpy()`；
  - 注意：
    - `torch.tensor()`可将 NumPy 中的 array 转换为 Tensor，且不共享内存；
    - 所有在CPU上的 Tensor （除了 CharTensor ）都⽀持与NumPy数组相互转换；
- ⽅法 to() ：可将 Tensor 在CPU和GPU（需要硬件⽀持）之间相互移动；
- `.cuda()`方法：将Tensor转换为GPU的Tensor，以便使用GPU加速；
- 3 种加法：
  - x + y；
  - t.add(x, y)；
  - t.add(x, y, out = result)；
- 函数名以`_`结尾的函数：均为 inplace 方式，即会修改原始数据；

## 1.3 自动求梯度

- 链式法则：即反向传播，用于更新网络参数；
- grad 在反向传播过程中是累加的，每次反向传播过程中都会累加此前的梯度，因此反向传播之前需将梯度清零；
- Variable：
  - 简单封装了Tensor，支持几乎所有的Tensor操作，`.backward()`方法可实现反向传播；
  - 与Tensor接口几乎一致，可等同使用；

# 2. 深度学习基础

## 2.1 自动求梯度

- 在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传⼊任何参数；否则，需要传⼊⼀个与 y 同形的 Tensor；
- grad在反向传播过程中是累加的(accumulated)，即每⼀次运⾏反向传播，梯度都会累加之前的梯度，所以⼀般在反向传播之前需把梯度清零；
<<<<<<< HEAD
- 如果想要修改 tensor 的数值，但是⼜不希望被 autograd 记录（即不会影响反向传播），那么可对 tensor.data 进⾏操作；
- 反向传播前梯度为None；
=======
- 如果想要修改 tensor 的数值，但是⼜不希望被 autograd 记录（即不会影响反向传播，不影响梯度），那么可对 tensor.data 进⾏操作；
>>>>>>> 4e03e2375767f04b2c29a1a54d601f1a304da0bf

## 2.2 线性回归

- 线性回归：
  - 假设输出与各个输⼊之间是线性关系；
  - 既可以⽤神经⽹络图表示线性回归，⼜可以⽤⽮量计算表示该模型；
  - 尽可能采⽤⽮量计算，以提升计算效率；
  - 作为⼀个单层神经⽹络，线性回归输出层中的神经元和输⼊层中各个输⼊完全连接。因此，线性回归的输出层⼜叫全连接层；
- 损失函数：计算预测值与真实值之间差值的函数，是以模型参数为参数的函数；
- 超参数：
  - 人为设定的 参数，而非通过模型训练学到的参数；
  - 调参：一般指反复试错调节超参数；
- 解析解和数值解：
  - 解析解：若模型和损失函数形式较为简单时，其误差最⼩化问题的解可⽤公式表示；
  - 数值解：大多数深度学习模型没有解析解，只能通过优化算法有限次迭代模型参数尽可能降低损失函数的值；
- ⼩批量随机梯度下降（mini-batch stochastic gradient descent）：mini-batch SGD；
  - 用途：一种求数值解的优化算法；
  - 实现方式：先选取⼀组模型参数的初始值，如随机选取；接下来对参数进⾏多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样⼀个由固定数⽬训练数据样本所组成的⼩批量（mini-batch） ，然后求⼩批量中数据样本的平均损失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积作为模型参数在本次迭代的减⼩量；
- 特征数：亦称特征数、特征向量维度，即神经网络中第一层输入值的个数；
- 输出层不涉及计算，因此计算神经网络层数时不统计输出层；
  - 线性回归可视为一个单层神经网络；
- 全连接层/稠密层：全连接层的每一个结点都与上一层的所有结点相连，将前边提取到的特征综合起来；
- 在一个周期（epoch）中，使用训练数据集中的所有样本各一次；

## 2.3 Softmax 回归

- Softmax 回归：
  - ⼀个单层神经⽹络，也是一个全连接层；
  - 将输出值变成值为正且和为1的概率分布；
  - 损失函数：交叉熵，适合衡量两个概率分布的差异；

## 2.4 多层感知机

- MLP：Multilayer Perceptron，多层感知机；
  - 在单层神经⽹络的基础上引⼊了⼀到多个隐藏层；
  - 多层感知机中的隐藏层和输出层都是全连接层；
  - 每个隐藏层的输出通过激活函数进⾏处理；
- 隐藏层的输出：亦称为隐藏层变量或隐藏变量；
- 为什么需要多层感知机中需要使用激活函数？
  - 全连接层只是对数据做仿射变换，⽽多个仿射变换的叠加仍然是⼀个仿射变换；
- 常见的激活函数：
  - ReLU 函数；
  - Sigmoid 函数：
    - 值域在0到1之间；
    - 当输⼊接近0时，sigmoid函数接近线性变换；
  - tanh 函数：
    - 值域在到-1和1之间；
    - 当输⼊接近0时，tanh函数接近线性变换；

## 2.5 模型选择、欠拟合与过拟合

- 一味地降低训练误差并不意味着泛化误差一定会降低，机器学习模型应关注降低泛化误差；
- 交叉验证集：用于选择模型；
- K 折交叉验证：
  - 将训练集切分为 K 个不重合的子数据集；
  - 做 K 次模型训练和验证；
  - 其中每次使用 K-1 个子数据集训练模型，使用 1 个子数据集验证模型；
  - 对 K 次训练误差和测试误差求均值，即为最终的训练误差和测试误差；
  - 意义：当训练数据不够时，预留大量的验证数据太过奢侈，故使用 K 折交叉验证；
- 泛化误差不会因为数据集的增大而增大；
- 权重衰减：
  - $\frac{\lambda}{2n}||w||^2$：其中$||w||^2$表示所有权重参数的平方和；
  - 等价于$L_2$范数正则化，常使得学到的权重参数接近于 0；
  - Pytorch 默认对权重参数和偏差参数同时衰减，可为权重参数和偏差参数分别构造优化器实例，从而单独设置仅对权重参数进行衰减；
  - 意义：为模型损失函数增加惩罚项，使得学到的模型参数值较小，避免过拟合；
- 丢弃法：Dropout；
  - 计算表达式中已对$h_i$除以$1-p$，因此丢弃法不改变输入的期望值；
  - 仅在模型训练过程中使用 Dropout，为得到确定的结果，在测试模型时不使用 Dropout；
  - 通常将靠近输入层的丢弃概率设置得较小；
  - 意义：设置丢弃概率，使隐层中的部分神经元值为 0，由于隐层神经元的丢弃是随机的，避免输出层的计算过度依赖某一个神经元，从而起到正则化的效果，避免过拟合；
- 代价函数：即目标函数，是“预测值与真实值之间的误差”与正则化项之和；
- 模型训练：
  - 反向传播过程会利用正向传播过程计算得到的中间变量，因此正向传播过程中计算得到的中间变量不能立即释放，故模型训练过程会比测试过程占用更多的内存；
  - Batch 越大，正向传播计算过程的中间变量越多，因此对较深的神经网络使用较大 Batch 运算时容易超内存；
  - 模型训练过程中，交替进行正向传播和反向传播；

## 2.6 数值稳定性和模型初始化

- 数值稳定性：网络层数过多使得模型数值稳定性变差；
  - 衰减：E.g. 若每层网络的参数值均小于1，则多层累乘后值接近于 0；
  - 爆炸：E.g. 若每层网络的参数值均大于1，则多层累乘后值接近于无穷；
- 模型初始化：
  - 模型参数应随机初始化，否则训练过程中，各个神经元的值将始终保持一致，等价于一个神经元；
  - PyTorch 中的 nn.module 模块参数均已完成了初始化，无需开发者自行考虑；
  - Xavier 随机初始化：
    - 一种随机初始化方法；
    - 意义：使得模型参数初始化之后，每层输出的方差不受该层输入的影响，每层梯度的方差也不受该层输出个数的影响；
# 3. 深度学习计算
## 3.1 模型构造

- Module 类：是 nn 模块提供的一个模型构造类，是所有神经网络模块的基类，可继承该类构造模型；
  - `__init__`函数：重载该函数，创建模型参数；
  - `forward`函数：重载该函数，定义正向传播计算过程；
  - 无需定义反向传播过程，调用`backward()`函数即可；
- Module 类的子类：
  - Sequential 类；
  - ModuleList 类；
  - ModuleDict 类；
- Sequential 类：
  - 可接收一个子模块的有序字典（OrderedDict），或一系列子模块作为参数，逐一添加 Module 实例；
  - 意义：若模型的前向计算为各层的简单串联，该类可简洁的定义模型；
- ModuleList 类：接收一个子模块的列表作为输入，也可类似于 List 那样进行 append 和 extend 操作；
- ModuleDict 类：接收一个子模块的字典作为输入，也可类似字典那样进行添加访问操作；

## 3.2 模型参数的访问、初始化和共享

- nn 的`init`模块：包含多种参数初始化方法；
- 访问模型参数：
  - Module 类的 parameters() 方法：以迭代器形式返回所有参数；
  - Module 类的 named_parameters() 方法：以迭代器形式返回所有参数名和参数；
    - 返回的参数名自动加上其所属的网络层数索引，以示区分；
    - 若网络仅有一层，则无索引前缀；
    - 对 net 使用方括号下标索引，可访问网络的任一层参数，索引从 0 开始；
    - 返回的 param 的类型为 torch.nn.parameter.Parameter，是 Tensor 的子类；
    - 如果一个 Tensor 是 Parameter，其将被自动添加到模型的参数列表里，而 Tensor 则不会；
- 参数初始化：
  - init.normal_()：以正态分布形式初始化参数；
  - init.constant_()：以常数初始化参数；
  - Parameter 类的 initialize 函数：
    - 与 Block 类的 initialize 函数使用方法相同；
    - 可对**某个**特定参数初始化；
- 共享模型参数的方法：
  - Module 类的 forward 函数中，若多次调用同一层，即可实现模型参数共享；
  - 若传入 Sequential 的模块是同一个 Module 实例，则其参数也是共享的；

## 3.3 自定义层

- 自定义不含模型参数的层：继承 Module 类，重载`__init__`函数和`forward`函数；
- 自定义含模型参数的层：
  - 应将参数定义为 Parameter、ParameterList 和 ParameterDict；
  - 因为如果一个 Tensor 是 Parameter，其将被自动添加到模型的参数列表里，而 Tensor 则不会；
  - 对 ParameterList 可进行 append 和 extend 操作，实现新增参数；

## 3.4 读取和存储

- 读写 Tensor：
  - save 函数：将单个 Tensor、多个 Tensor 组成的列表、从字符串映射到 Tensor 的字典保存到`.pt`文件中；
  - load 函数：将`.pt`文件中的数据读回内存；
- 读写模型：
  - state_dict：是一个从参数名映射到参数 Tensor 的字典对象；
  - 具有可学习参数的层、优化器均有 state_dict，其中包含参数的 Tensor 和优化器的状态以及优化器使用的超参数信息；
  - 保存和加载模型的方法：
    - 仅保存和加载模型参数 state_dict（推荐）；
      - save 函数；
      - load_state_dict 函数；
    - 保存和加载整个模型；

## 3.5 GPU 计算

- 默认情况下，PyTorch 在内存中创建数据，使用 CPU 计算；
  - 可在创建 Tensor 的时候指定 CPU 或 GPU；
  - GPU 上 Tensor 的计算结果仍位于 GPU 上；
  - 位于 CPU 上的数据不能直接和 GPU 上的数据进行运算，位于不同 GPU 上的数据也不能直接进行运算；
  - 即参与计算的所有数据应位于 CPU 上或同一块显卡上；
- 使用`.cuda()`可将 CPU 上的 Tensor 转换到 GPU 上；
  - 如果有多块 GPU，可用 .cuda(i) 表示第 i 块 GPU，索引从 0 开始；
  - .cuda() 和 .cuda(0) 等价；
- 使用`.cuda()`可将模型及其所有参数转换到 GPU 上；

# 4. 卷积神经网络

## 4.1 二维卷积层

- 卷积层输入输出的数据维度：样本、通道、高、宽；
- 二维互相关运算：
  - 卷积核：亦称过滤器；
  - 卷积核大小：亦称卷积核窗口、卷积窗口，常取奇数，E.g. 1、3、5、7；
  - 计算过程：输入数组中对应位置的元素与卷积核中对应的元素相乘并求和；
- 二维卷积层：
  - 有两个维度，常用于处理图像数据；
  - 实现方式：
    - 将输入和卷积核做互相关运算，加上一个标量偏差即为输出；
    - 卷积核和标量偏差通过训练得到；
  - $p\times q$卷积层：卷积核大小为$p\times q$；
  - 意义：计算输入数据和卷积核之间的互相关性；
  - 作用：检测图像中物体的边缘，表征局部空间；
- 特征图和感受野：
  - 特征图：二维卷积层输出的二维数组，可视为输入数据在空间维度上某一级的表征；
  - 感受野：对于第 i 层中的某个元素，第 j 层中参与计算得到该元素的所有值所在的位置，即为第 i 层中的某个元素的感受野；
  - 可通过更深的 CNN 使特征图中单个元素的感受野变得更加宽阔，从而捕捉输入上更大尺寸的特征；

## 4.2 填充和步幅

- 卷积层的超参数:
  - 卷积层的输出形状取决于输入数据形状、卷积核形状、填充、步幅；
  - 填充和步幅是卷积层的两个超参数，默认情况下填充为0，步幅为1；
- 填充：
  - 若两端填充个数相同，并使得输入输出的高和宽相同时，即可推得输出$Y[i,j]$是以$X[i,j]$为中心的窗口与卷积核进行互相关运算得到的；
  - 填充为$(p_h,p_w)$：输入的高和宽两侧的填充数分别为$p_h$、$p_w$；
  - 填充为$p$：输入的高和宽两侧的填充数均为$p$；
  - 意义：使输入输出的高和宽相同；
- 步幅：
  - 减小输出 Tensor 的宽和高；
  - 步幅为$(s_h,s_w)$：在高和宽上的步幅分别为$s_h$、$s_w$；
  - 步幅为$s$：在高和宽上的步幅均为$s$；
- dilation：
  - 该参数增大感受野，但不增加计算量；
  - 改变了互相关运算中，输入数据与卷积核之间的对应关系，具体对应关系可通过搜索引擎查找示意图；

##  4.3 多输入通道和多输出通道

- RGB 图像除了高和宽之外，还有 3 个颜色通道，该维度称为通道维（channel ）；
- 多输入通道：输入 Tensor 的通道维大于 1；
- 单输出通道：卷积核在通道方向上的维度若与输入数据的通道维度相等，则输出数据的通道维度为 1；
- 多输出通道：取 n 个卷积核，分别对输入数据进行互相关运算，得到 n 个单通道输出，将其叠加在一起即为多通道输出；
- $1\times 1$卷积层：
  - $1\times 1$卷积层失去了卷积层可识别高和宽维度上相邻元素构成的模式的功能；
  - $1\times 1$卷积层可保持输入输出数据的高和宽维度相同；
  - 若将通道维视为特征，将高和宽维度上的元素视为数据样本，则$1\times 1$卷积层的作用于全连接层等价；
  - 意义：可减少 Tensor 的通道数，从而降低模型复杂度；

## 4.4 池化层

- 池化层：

  - 意义：缓解卷积层对位置的过度敏感性；

  - 池化层对每个通道分别池化，因此输入输出数据的通道数相等；

- 池化运算：

  - 最大池化：取池化窗口内所有值中的最大值；
  - 平均池化：取池化窗口内所有值中的平均值；
  - $p\times q$池化运算：即池化窗口的大小为$p\times q$；

- nn.MaxPool2d()：

  - 二维最大池化；
  - 该函数默认步幅与池化窗口的大小相同；

## 4.5 卷积神经网络（LeNet）

- 卷积神经网络：即含有卷积层的网络；
- LeNet：交替使用卷积层、池化层、全连接层，实现图像分类；
- 卷积层块：
  - 卷积层：识别图像中的空间模式；
  - 池化层：降低卷积层对位置的敏感性；
- flatten：
  - 卷积层块的输出形状为（批量大小、通道、高、宽），当卷积层块的输出数据传入全连接层块时，全连接层块将小批量中的每个样本变平（flatten）；
  - 即将全连接层的输入形状调整为二维，第一维至批量大小，第二维是每个向量变平之后的表示，该向量长度为通道、高和宽的乘积；

## 4.6 深度卷积神经网络（AlexNet）

- 多层神经网络可能学到数据的多级表征，并逐级表示越来越抽象的概念或模式；
- 设计思路：以卷积层抽取空间特征，以全连接层输出分类结果；

## 4.7 批量归一化

- 数据标准化处理：处理后的数据均值为0，标准差为1；
- 浅层模型：使用数据标准化预处理即可；
- 深层模型：应使用批量归一化，避免网络各层输出的中间结果出现剧烈变化，避免出现这种数值不稳定的情况；
- Batch Normalization：批量归一化；
  - 利用小批量上的均值和标准差调整神经网络的中间输出；
  - 批量归一化层和 Dropout 层一样，在训练过程和测试过程中的计算结果不同；
- 全连接层的批量归一化实现方法：
  - 仿射变换（使用前一层中的每个数值计算得到后一层）、批量归一化、激活函数；
  - 模型参数：拉伸参数、偏移参数；
  - PyTorch 中的类：
    - nn.BatchNorm1d；
    - 需指定输入的 num_features 参数值，即为batch_size * num_features * height * width，即为特征数/channel数；
- 卷积层的批量归一化实现方法：
  - 卷积计算、批量归一化、激活函数；
  - 可将批量设置得稍大一些，从而使批量内样本的均值和方差计算较为准确；
  - 模型参数：
    - 每个通道有各自的拉伸参数和偏移参数；
    - 对每个通道分别做批量归一化；
  - PyTorch 中的类：
    - nn.BatchNorm2d；
    - 需指定输入的 num_features 参数值，即为batch_size * num_features * height * width，即为特征数/channel数；

## 4.8 残差网络（Resnet）和稠密网络（DenseNet）

- 残差网络：A+B，对应数值相加，通道维度不变；
- 稠密网络：A和B在通道维上叠加，故称为稠密连接；
  - 稠密块：定义输入和输出之间的连接关系；
  - 过渡层：控制通道数，使之不变得较大；
    - 通过$1\times 1$卷积层减小通道数；
    - 使用步幅为 2 的平均池化层令高和宽减半；

# 5. 循环神经网络

## 5.1 语言模型和 n 元语法

- $P(w_1)$：单词$w_1$在训练集中的词频（出现的频率）；
- $P(W_2|W_1)=\frac{P(w_1, w_2)}{P(w_1)}$：即单词$w_1$和$w_2$相邻的概率除以单词$w_1$在训练集中的词频；
- n-grams：n 元语法，n 阶马尔可夫链，亦称马尔可夫假设；
  - 一个词的出现仅与 n 个词相关（第 i 个词的出现仅与前 n - 1 个词相关）；
  - 当 n 取值为 1、2、3 时，称为一元语法、二元语法、三元语法；
  - 意义：权衡计算复杂度和模型的准确性；

## 5.2 循环神经网络

- 循环神经网络：使用循环计算的网络即称为循环神经网络；
  - 第 i 个时间步的输出由当前时间步的输入和第 i-1 个时间步的隐藏变量共同决定；
  - 循环神经网络的隐藏状态可捕获前 i-1 个时间步的序列信息；
  - 循环神经网络模型参数的数量不随时间步的增长而增加，因为每一个时间步的参数为同一组变量；

## 5.3 循环神经网络的实现

- 时序数据的采样方式：
  - 随机采样：每次从数据集中随机采样一个 batch，相邻的两个 batch 在原始序列上的位置不一定相邻；
  - 相邻采样；
- LSTM：
  - 输出的形状：时间步数、批量大小、输入个数；
  - 隐藏状态 h 的形状：层数、批量大小、隐藏单元个数；
  - 隐藏状态是一个元组 (h, c)：
    - hidden state；
    - cell state；

# 3. 深度学习计算

## 3.1 模型构造

- 模型构造：
  - 可通过继承 Module 类来构造模型；
  - Sequential 、 ModuleList 、 ModuleDict 类都继承⾃ Module 类；
  - 虽然 Sequential 等类可以使模型构造更加简单，但直接继承 Module 类可以极⼤地拓展模型构
    造的灵活性；
- Module 类：
  - 是一个模型构造类，是所有神经⽹络模块的基类，可以继承它来定义我们想要的模型；
  - 可以实例化该类（继承 Module 类的子类）得到模型变量`net`；
  - 重载`__init__`函数：声明带有模型参数的层；
  - 重载`forward`函数：定义前向计算，即正向传播；
  - ⽆须定义反向传播函数，系统可通过`backward()`函数求解；
- Sequential 类：
  - 当模型的前向计算为简单串联各个层的计算时， Sequential 类可通过更加简单的⽅式定义模型；
  - 它可以接收⼀个⼦模块的有序字典（OrderedDict）或者⼀系列⼦模块作为参数来逐⼀添加 Module 实例，⽽模型的前向计算就是将这些实例按添加的顺序逐⼀计算；
  - 对于使⽤ Sequential 类构造的神经⽹络，可以通过⽅括号 [] 索引来访问⽹络的任⼀层，索引0表示隐藏层为 Sequential 实例最先添加的层；
- ModuleList 类：接收⼀个⼦模块的列表作为输⼊，然后也可以类似List那样进⾏append和extend操作；
- ModuleDict 类：接收⼀个⼦模块的字典作为输⼊, 然后也可以类似字典那样进⾏添加访问操作；

## 3.2 模型参数的访问、初始化和共享

- 访问模型参数：
  - 可以通过 Module 类的 parameters() 或者 named_parameters ⽅法来访问所有参数（以迭代器的形式返回），后者除了返回参数 Tensor 外还会返回其名字；
  - param 的类型为 torch.nn.parameter.Parameter ，这是 Tensor 的⼦类；
  - param 会⾃动被添加到模型的参数列表⾥，而 Tensor 不会⾃动被添加到模型的参数列表⾥；
- 参数初始化：
  - 如果只想对某个特定参数进⾏初始化，可以调⽤ Parameter 类的 initialize 函数，它与 Block
    类提供的 initialize 函数的使⽤⽅法⼀致；
  - init.norml_()：对参数进行正态分布初始化；
  - init.constant_()：使⽤常数来初始化权重参数；

# ==待整理的内容==

- 损失函数：
  - 线性回归：平方损失函数；
  - Softmax 回归：交叉熵损失函数；
- PyTorch：
  - linear()：用于设置全连接层；

# TODO

- 学习计划：
<<<<<<< HEAD
  - 总页数：364 页；
  - 学习周期：每天10页，12月24日-01月27日；
  - 当前进度：Page 96， 3.12 已完成；
  - 今日进度：01 月 01 日，学完 Chapter 4.5，Page 134；
  - 需要追赶的进度：15 页；
=======
  - 总页数：364 页 - 23 页（Chapter 9）= 341 页；
  - 学习周期：每天15页，12月24日-01月16日；
  - 当前进度：Page 217， Chapter 6.5 已完成；
  - 今日进度：01 月 08 日，学完 Chapter 6.8，Page 234；
>>>>>>> 4e03e2375767f04b2c29a1a54d601f1a304da0bf

# References

[1] Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola. Dive into Deep Learning[M]. 人民邮电出版社, 2019. 